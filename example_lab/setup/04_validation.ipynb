{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Setup Validation\n",
    "\n",
    "This interactive notebook provides comprehensive validation procedures to ensure your MADSci laboratory setup is functioning correctly.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have completed:\n",
    "- ✅ **01_service_orchestration.ipynb** - All MADSci services are running\n",
    "- ✅ **02_resource_templates.ipynb** - Resource templates are created\n",
    "- ✅ **03_initial_resources.ipynb** - Lab resources are provisioned\n",
    "\n",
    "## Overview\n",
    "\n",
    "Validation covers multiple layers:\n",
    "- **Service connectivity and health**\n",
    "- **Resource management functionality**  \n",
    "- **Context propagation and ownership**\n",
    "- **Workflow execution capabilities**\n",
    "- **Data flow and persistence**\n",
    "- **Integration testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # MADSci clients\n",
    "    from madsci.client.data_client import DataClient\n",
    "    from madsci.client.event_client import EventClient\n",
    "    from madsci.client.experiment_client import ExperimentClient\n",
    "    from madsci.client.resource_client import ResourceClient\n",
    "    from madsci.client.workcell_client import WorkcellClient\n",
    "    from madsci.common.context import MadsciContext\n",
    "\n",
    "    # Types and utilities\n",
    "    from madsci.common.types.context_types import OwnershipInfo\n",
    "    from madsci.common.types.resource_types import Resource\n",
    "    from madsci.common.types.resource_types.resource_enums import ResourceStatusEnum\n",
    "\n",
    "    print(\"✅ All MADSci modules imported successfully\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    print(\"Make sure MADSci is installed and services are running\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for validation\n",
    "import builtins\n",
    "import contextlib\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "# Database connectivity (optional)\n",
    "try:\n",
    "    import psycopg2\n",
    "\n",
    "    postgresql_available = True\n",
    "except ImportError:\n",
    "    postgresql_available = False\n",
    "    print(\n",
    "        \"ℹ️  psycopg2 not available - PostgreSQL direct connectivity tests will be skipped\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    import pymongo\n",
    "\n",
    "    mongodb_available = True\n",
    "except ImportError:\n",
    "    mongodb_available = False\n",
    "    print(\n",
    "        \"ℹ️  pymongo not available - MongoDB direct connectivity tests will be skipped\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    import redis\n",
    "\n",
    "    redis_available = True\n",
    "except ImportError:\n",
    "    redis_available = False\n",
    "    print(\"ℹ️  redis not available - Redis direct connectivity tests will be skipped\")\n",
    "\n",
    "print(\"🔧 Validation tools initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Test Framework\n",
    "\n",
    "Let's set up our validation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class ValidationFramework:\n",
    "    \"\"\"Framework for running and tracking validation tests.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize validation framework.\"\"\"\n",
    "        self.results = {}\n",
    "        self.start_time = datetime.now()\n",
    "        self.test_context = OwnershipInfo(\n",
    "            node=\"validation_framework\",\n",
    "            experiment=\"system_validation\",\n",
    "            user=\"test_user\",\n",
    "        )\n",
    "\n",
    "    def run_test(\n",
    "        self,\n",
    "        test_name: str,\n",
    "        test_function: callable[..., bool],\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> bool:\n",
    "        \"\"\"Run a validation test and record results.\"\"\"\n",
    "        print(f\"\\n🧪 Running: {test_name}\")\n",
    "        print(\"-\" * (len(test_name) + 12))\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = test_function(*args, **kwargs)\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            self.results[test_name] = {\n",
    "                \"status\": \"PASS\" if result else \"FAIL\",\n",
    "                \"duration\": duration,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"result\": result,\n",
    "            }\n",
    "\n",
    "            status_icon = \"✅\" if result else \"❌\"\n",
    "            print(\n",
    "                f\"{status_icon} {test_name}: {'PASSED' if result else 'FAILED'} ({duration:.2f}s)\"\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            duration = time.time() - start_time\n",
    "            self.results[test_name] = {\n",
    "                \"status\": \"ERROR\",\n",
    "                \"duration\": duration,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "            print(f\"❌ {test_name}: ERROR - {e} ({duration:.2f}s)\")\n",
    "            return False\n",
    "\n",
    "    def get_summary(self) -> dict[str, Any]:\n",
    "        \"\"\"Get validation summary.\"\"\"\n",
    "        total = len(self.results)\n",
    "        passed = len([r for r in self.results.values() if r[\"status\"] == \"PASS\"])\n",
    "        failed = len([r for r in self.results.values() if r[\"status\"] == \"FAIL\"])\n",
    "        errors = len([r for r in self.results.values() if r[\"status\"] == \"ERROR\"])\n",
    "\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"errors\": errors,\n",
    "            \"success_rate\": (passed / total * 100) if total > 0 else 0,\n",
    "            \"duration\": (datetime.now() - self.start_time).total_seconds(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize validation framework\n",
    "validator = ValidationFramework()\n",
    "print(\"🔍 Validation framework initialized\")\n",
    "print(\n",
    "    f\"Test context: {validator.test_context.node} / {validator.test_context.experiment}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Service Health Validation\n",
    "\n",
    "First, let's validate that all MADSci services are healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service health validation\n",
    "def validate_service_health() -> bool:\n",
    "    \"\"\"Validate that all MADSci services are healthy.\"\"\"\n",
    "    services = [\n",
    "        (\"Event Manager\", 8001),\n",
    "        (\"Experiment Manager\", 8002),\n",
    "        (\"Resource Manager\", 8003),\n",
    "        (\"Data Manager\", 8004),\n",
    "        (\"Workcell Manager\", 8005),\n",
    "    ]\n",
    "\n",
    "    healthy_services = 0\n",
    "    total_services = len(services)\n",
    "\n",
    "    for service_name, port in services:\n",
    "        try:\n",
    "            response = requests.get(f\"http://localhost:{port}/health\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"   ✅ {service_name} (port {port}) - Healthy\")\n",
    "                healthy_services += 1\n",
    "            else:\n",
    "                print(\n",
    "                    f\"   ❌ {service_name} (port {port}) - Status {response.status_code}\"\n",
    "                )\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"   ❌ {service_name} (port {port}) - Unreachable: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n   📊 Service Health: {healthy_services}/{total_services} services healthy\"\n",
    "    )\n",
    "    return healthy_services == total_services\n",
    "\n",
    "\n",
    "# Run service health validation\n",
    "validator.run_test(\"Service Health Check\", validate_service_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connectivity Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connectivity validation\n",
    "def validate_database_connectivity() -> bool:\n",
    "    \"\"\"Validate database connectivity.\"\"\"\n",
    "    databases_healthy = 0\n",
    "    total_databases = 3\n",
    "\n",
    "    # PostgreSQL validation\n",
    "    if postgresql_available:\n",
    "        try:\n",
    "            # ! Don't hard code your production passwords, kids!\n",
    "            conn = psycopg2.connect(\n",
    "                host=\"localhost\",\n",
    "                port=5432,\n",
    "                database=\"madsci\",\n",
    "                user=\"madsci\",\n",
    "                password=\"madsci_pass\",  # noqa\n",
    "            )\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT 1\")\n",
    "            cursor.fetchone()\n",
    "            conn.close()\n",
    "            print(\"   ✅ PostgreSQL - Connected\")\n",
    "            databases_healthy += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ PostgreSQL - Failed: {e}\")\n",
    "    else:\n",
    "        print(\"   ⏭️  PostgreSQL - Skipped (psycopg2 not available)\")\n",
    "        total_databases -= 1\n",
    "\n",
    "    # MongoDB validation\n",
    "    if mongodb_available:\n",
    "        try:\n",
    "            client = pymongo.MongoClient(\n",
    "                \"mongodb://localhost:27017\", serverSelectionTimeoutMS=3000\n",
    "            )\n",
    "            client.admin.command(\"ping\")\n",
    "            print(\"   ✅ MongoDB - Connected\")\n",
    "            databases_healthy += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ MongoDB - Failed: {e}\")\n",
    "    else:\n",
    "        print(\"   ⏭️  MongoDB - Skipped (pymongo not available)\")\n",
    "        total_databases -= 1\n",
    "\n",
    "    # Redis validation\n",
    "    if redis_available:\n",
    "        try:\n",
    "            r = redis.Redis(host=\"localhost\", port=6379, decode_responses=True)\n",
    "            r.ping()\n",
    "            print(\"   ✅ Redis - Connected\")\n",
    "            databases_healthy += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Redis - Failed: {e}\")\n",
    "    else:\n",
    "        print(\"   ⏭️  Redis - Skipped (redis not available)\")\n",
    "        total_databases -= 1\n",
    "\n",
    "    print(\n",
    "        f\"\\n   📊 Database Health: {databases_healthy}/{total_databases} databases connected\"\n",
    "    )\n",
    "    return databases_healthy == total_databases if total_databases > 0 else True\n",
    "\n",
    "\n",
    "# Run database connectivity validation\n",
    "validator.run_test(\"Database Connectivity\", validate_database_connectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Client Initialization Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client initialization validation\n",
    "def validate_client_initialization() -> bool:\n",
    "    \"\"\"Validate that all MADSci clients can be initialized.\"\"\"\n",
    "    clients = {}\n",
    "    client_classes = [\n",
    "        (\"Resource\", ResourceClient, {}),\n",
    "        (\"Event\", EventClient, {}),\n",
    "        (\"Workcell\", WorkcellClient, {}),\n",
    "        (\"Experiment\", ExperimentClient, {}),\n",
    "        (\"Data\", DataClient, {}),\n",
    "    ]\n",
    "\n",
    "    successful_clients = 0\n",
    "\n",
    "    for name, client_class, kwargs in client_classes:\n",
    "        try:\n",
    "            client = client_class(**kwargs)\n",
    "            clients[name.lower()] = client\n",
    "            print(f\"   ✅ {name} Client - Initialized\")\n",
    "            successful_clients += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {name} Client - Failed: {e}\")\n",
    "\n",
    "    # Store clients for later use\n",
    "    validator.clients = clients\n",
    "\n",
    "    print(\n",
    "        f\"\\n   📊 Client Initialization: {successful_clients}/{len(client_classes)} clients initialized\"\n",
    "    )\n",
    "    return successful_clients == len(client_classes)\n",
    "\n",
    "\n",
    "# Run client initialization validation\n",
    "validator.run_test(\"Client Initialization\", validate_client_initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Template Operations Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madsci.common.types.resource_types import Container\n",
    "from madsci.common.types.resource_types.resource_enums import ContainerTypeEnum\n",
    "\n",
    "\n",
    "# Template operations validation\n",
    "def validate_template_operations() -> Optional[bool]:\n",
    "    \"\"\"Test complete template lifecycle operations.\"\"\"\n",
    "    if \"resource\" not in validator.clients:\n",
    "        print(\"   ❌ Resource client not available\")\n",
    "        return False\n",
    "\n",
    "    client = validator.clients[\"resource\"]\n",
    "    test_template_name = \"validation_test_template\"\n",
    "\n",
    "    try:\n",
    "        # Clean up any existing test template\n",
    "        with contextlib.suppress(Exception):\n",
    "            client.delete_template(test_template_name)\n",
    "\n",
    "        # Test 1: Create template\n",
    "        test_resource = Container(\n",
    "            resource_name=\"ValidationPlate\",\n",
    "            base_type=ContainerTypeEnum.container,\n",
    "            rows=8,\n",
    "            columns=12,\n",
    "            capacity=96,\n",
    "        )\n",
    "\n",
    "        client.create_template(\n",
    "            resource=test_resource,\n",
    "            template_name=test_template_name,\n",
    "            description=\"Template for validation testing\",\n",
    "            required_overrides=[\"resource_name\"],\n",
    "            tags=[\"validation\", \"test\"],\n",
    "        )\n",
    "        print(\"   ✅ Template creation - Success\")\n",
    "\n",
    "        # Test 2: List templates\n",
    "        templates = client.list_templates(tags=[\"validation\"])\n",
    "        if len(templates) >= 1:\n",
    "            print(\"   ✅ Template listing - Success\")\n",
    "        else:\n",
    "            print(\"   ❌ Template listing - No validation templates found\")\n",
    "            return False\n",
    "\n",
    "        # Test 3: Get template info\n",
    "        template_info = client.get_template_info(test_template_name)\n",
    "        if template_info and \"description\" in template_info:\n",
    "            print(\"   ✅ Template info retrieval - Success\")\n",
    "        else:\n",
    "            print(\"   ❌ Template info retrieval - Failed\")\n",
    "            return False\n",
    "\n",
    "        # Test 4: Create resource from template\n",
    "        resource = client.create_resource_from_template(\n",
    "            template_name=test_template_name,\n",
    "            resource_name=\"ValidationPlate_001\",\n",
    "            overrides={\"owner\": validator.test_context.model_dump()},\n",
    "            add_to_database=True,\n",
    "        )\n",
    "        print(\"   ✅ Resource creation from template - Success\")\n",
    "\n",
    "        # Test 5: Clean up\n",
    "        client.delete_resource(resource.resource_id)\n",
    "        client.delete_template(test_template_name)\n",
    "        print(\"   ✅ Template cleanup - Success\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Template operations failed: {e}\")\n",
    "        # Attempt cleanup\n",
    "        with contextlib.suppress(builtins.BaseException):\n",
    "            client.delete_template(test_template_name)\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run template operations validation\n",
    "validator.run_test(\"Template Operations\", validate_template_operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resource Lifecycle Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource lifecycle validation\n",
    "def validate_resource_lifecycle() -> Optional[bool]:\n",
    "    \"\"\"Test complete resource lifecycle operations.\"\"\"\n",
    "    result = True\n",
    "    if \"resource\" not in validator.clients:\n",
    "        print(\"   ❌ Resource client not available\")\n",
    "        return False\n",
    "\n",
    "    client = validator.clients[\"resource\"]\n",
    "\n",
    "    try:\n",
    "        # Test 1: Create resource\n",
    "        test_resource = Resource(\n",
    "            resource_name=\"LifecycleTestResource\",\n",
    "            resource_class=\"TestResource\",\n",
    "            attributes={\"test_value\": 42},\n",
    "            owner=validator.test_context,\n",
    "        )\n",
    "\n",
    "        created_resource = client.add_resource(test_resource)\n",
    "        resource_id = created_resource.resource_id\n",
    "        print(f\"   ✅ Resource creation - Success (ID: {resource_id[:8]}...)\")\n",
    "\n",
    "        # Test 2: Get resource\n",
    "        retrieved_resource = client.get_resource(resource_id)\n",
    "        if (\n",
    "            retrieved_resource\n",
    "            and retrieved_resource.resource_name == \"LifecycleTestResource\"\n",
    "        ):\n",
    "            print(\"   ✅ Resource retrieval - Success\")\n",
    "        else:\n",
    "            print(\"   ❌ Resource retrieval - Failed\")\n",
    "            result = False\n",
    "\n",
    "        # Test 3: Update resource\n",
    "        updated_resource = client.update_resource(\n",
    "            resource_id,\n",
    "            {\n",
    "                \"attributes\": {\"test_value\": 100, \"updated\": True},\n",
    "                \"status\": ResourceStatusEnum.in_use,\n",
    "            },\n",
    "        )\n",
    "        if updated_resource.attributes.get(\"test_value\") == 100:\n",
    "            print(\"   ✅ Resource update - Success\")\n",
    "        else:\n",
    "            print(\"   ❌ Resource update - Failed\")\n",
    "            result = False\n",
    "\n",
    "        # Test 4: List resources with filters\n",
    "        resources = client.list_resources(owner_node=\"validation_framework\")\n",
    "        if any(r.resource_id == resource_id for r in resources):\n",
    "            print(\"   ✅ Resource listing/filtering - Success\")\n",
    "        else:\n",
    "            print(\"   ❌ Resource listing/filtering - Failed\")\n",
    "            result = False\n",
    "\n",
    "        # Test 5: Delete resource\n",
    "        success = client.delete_resource(resource_id)\n",
    "        if success:\n",
    "            # Verify deletion\n",
    "            deleted_resource = client.get_resource(resource_id)\n",
    "            if deleted_resource is None:\n",
    "                print(\"   ✅ Resource deletion - Success\")\n",
    "            else:\n",
    "                print(\"   ❌ Resource deletion verification - Failed\")\n",
    "                result = False\n",
    "        else:\n",
    "            print(\"   ❌ Resource deletion - Failed\")\n",
    "            result = False\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Resource lifecycle failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run resource lifecycle validation\n",
    "validator.run_test(\"Resource Lifecycle\", validate_resource_lifecycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Context Propagation Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context propagation validation\n",
    "def validate_context_propagation() -> bool:\n",
    "    \"\"\"Test context propagation across services.\"\"\"\n",
    "    context_tests_passed = 0\n",
    "    total_context_tests = 2\n",
    "\n",
    "    # Test 1: Event logging with context\n",
    "    if \"event\" in validator.clients:\n",
    "        try:\n",
    "            event_client = validator.clients[\"event\"]\n",
    "\n",
    "            # Create context\n",
    "            context = MadsciContext(\n",
    "                ownership=validator.test_context, lab_id=\"01JVDFED2K18FVF0E7JM7SX09F\"\n",
    "            )\n",
    "\n",
    "            event_client.log_event(\n",
    "                event_type=\"context_validation\",\n",
    "                message=\"Testing context propagation\",\n",
    "                context=context,\n",
    "            )\n",
    "            print(\"   ✅ Event logging with context - Success\")\n",
    "            context_tests_passed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Event logging with context - Failed: {e}\")\n",
    "    else:\n",
    "        print(\"   ⏭️  Event logging with context - Skipped (client not available)\")\n",
    "        total_context_tests -= 1\n",
    "\n",
    "    # Test 2: Resource operations with context\n",
    "    if \"resource\" in validator.clients:\n",
    "        try:\n",
    "            resource_client = validator.clients[\"resource\"]\n",
    "\n",
    "            # Create resource with context\n",
    "            test_resource = Resource(\n",
    "                resource_name=\"ContextTestResource\", owner=validator.test_context\n",
    "            )\n",
    "\n",
    "            created = resource_client.add_resource(test_resource)\n",
    "            if created.owner.node == validator.test_context.node:\n",
    "                print(\"   ✅ Resource creation with context - Success\")\n",
    "                context_tests_passed += 1\n",
    "\n",
    "                # Clean up\n",
    "                resource_client.delete_resource(created.resource_id)\n",
    "            else:\n",
    "                print(\"   ❌ Resource creation with context - Context not preserved\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Resource operations with context - Failed: {e}\")\n",
    "    else:\n",
    "        print(\"   ⏭️  Resource operations with context - Skipped (client not available)\")\n",
    "        total_context_tests -= 1\n",
    "\n",
    "    print(\n",
    "        f\"\\n   📊 Context Propagation: {context_tests_passed}/{total_context_tests} tests passed\"\n",
    "    )\n",
    "    return (\n",
    "        context_tests_passed == total_context_tests if total_context_tests > 0 else True\n",
    "    )\n",
    "\n",
    "\n",
    "# Run context propagation validation\n",
    "validator.run_test(\"Context Propagation\", validate_context_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Node Communication Validation (Optional)\n",
    "\n",
    "This test validates communication with laboratory nodes if they are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node communication validation\n",
    "def validate_node_communication() -> bool:\n",
    "    \"\"\"Test node registration and communication.\"\"\"\n",
    "    # Expected nodes based on example_lab configuration\n",
    "    expected_nodes = [\n",
    "        {\"name\": \"liquidhandler_1\", \"port\": 6001},\n",
    "        {\"name\": \"platereader_1\", \"port\": 6002},\n",
    "        {\"name\": \"robotarm_1\", \"port\": 6003},\n",
    "    ]\n",
    "\n",
    "    active_nodes = 0\n",
    "    total_expected = len(expected_nodes)\n",
    "\n",
    "    for node in expected_nodes:\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"http://localhost:{node['port']}/health\", timeout=3\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                print(f\"   ✅ Node {node['name']} - Active\")\n",
    "                active_nodes += 1\n",
    "\n",
    "                # Test actions endpoint\n",
    "                try:\n",
    "                    actions_response = requests.get(\n",
    "                        f\"http://localhost:{node['port']}/actions\", timeout=3\n",
    "                    )\n",
    "                    if actions_response.status_code == 200:\n",
    "                        actions = actions_response.json()\n",
    "                        print(f\"      • Actions available: {len(actions)}\")\n",
    "                except Exception:\n",
    "                    print(\"      • Actions endpoint not accessible\")\n",
    "\n",
    "            else:\n",
    "                print(\n",
    "                    f\"   ❌ Node {node['name']} - Unhealthy (status: {response.status_code})\"\n",
    "                )\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"   ⏭️  Node {node['name']} - Not running (optional)\")\n",
    "\n",
    "    print(f\"\\n   📊 Node Communication: {active_nodes}/{total_expected} nodes active\")\n",
    "\n",
    "    # Consider this test successful if at least some nodes are running or none are expected\n",
    "    return True  # Nodes are optional for basic validation\n",
    "\n",
    "\n",
    "# Run node communication validation\n",
    "validator.run_test(\"Node Communication\", validate_node_communication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Workflow Execution Validation (Optional)\n",
    "\n",
    "This test validates basic workflow submission if nodes are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow execution validation\n",
    "def validate_workflow_execution() -> Optional[bool]:\n",
    "    \"\"\"Test basic workflow submission and monitoring.\"\"\"\n",
    "    if \"workcell\" not in validator.clients:\n",
    "        print(\"   ❌ Workcell client not available\")\n",
    "        return False\n",
    "\n",
    "    client = validator.clients[\"workcell\"]\n",
    "\n",
    "    workflow_path = _find_example_workflow_path()\n",
    "    if workflow_path is None:\n",
    "        print(\"   ⏭️  Workflow execution - Skipped (no example workflow found)\")\n",
    "        return True  # Skip if no workflow available\n",
    "\n",
    "    try:\n",
    "        workflow_id = _submit_workflow(client, workflow_path)\n",
    "        if not workflow_id:\n",
    "            print(\"   ❌ Workflow submission - No workflow ID returned\")\n",
    "            return False\n",
    "\n",
    "        finished = _monitor_workflow(client, workflow_id)\n",
    "        if not finished:\n",
    "            print(\"   ⚠️  Workflow execution - Timeout (workflow may still be running)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Workflow execution failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def _find_example_workflow_path() -> Optional[Path]:\n",
    "    \"\"\"Find the example workflow file path.\"\"\"\n",
    "    workflow_path = Path(\"../workflows/example_workflow.workflow.yaml\")\n",
    "    if workflow_path.exists():\n",
    "        return workflow_path\n",
    "    workflow_path = Path(\"workflows/example_workflow.workflow.yaml\")\n",
    "    if workflow_path.exists():\n",
    "        return workflow_path\n",
    "    return None\n",
    "\n",
    "\n",
    "def _submit_workflow(client: WorkcellClient, workflow_path: Path) -> Optional[str]:\n",
    "    \"\"\"Submit the workflow and return its ID.\"\"\"\n",
    "    workflow_id = client.submit_workflow(\n",
    "        str(workflow_path), context=validator.test_context\n",
    "    )\n",
    "    if workflow_id:\n",
    "        print(f\"   ✅ Workflow submission - Success (ID: {workflow_id[:8]}...)\")\n",
    "        return workflow_id\n",
    "    return None\n",
    "\n",
    "\n",
    "def _monitor_workflow(\n",
    "    client: WorkcellClient, workflow_id: str, max_wait: int = 10\n",
    ") -> bool:\n",
    "    \"\"\"Monitor workflow status for completion.\"\"\"\n",
    "    start_time = time.time()\n",
    "    finished = False\n",
    "    while time.time() - start_time < max_wait and not finished:\n",
    "        try:\n",
    "            status = client.get_workflow_status(workflow_id)\n",
    "            if status.state in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "                finished = True\n",
    "                if status.state == \"completed\":\n",
    "                    print(\"   ✅ Workflow execution - Completed\")\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"   ⚠️  Workflow execution - Finished with state: {status.state}\"\n",
    "                    )\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Workflow monitoring - Error: {e}\")\n",
    "            break\n",
    "    return finished\n",
    "\n",
    "\n",
    "# Run workflow execution validation\n",
    "validator.run_test(\"Workflow Execution\", validate_workflow_execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Existing Lab Resources Validation\n",
    "\n",
    "Let's validate that resources from previous setup steps are accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing lab resources validation\n",
    "\n",
    "\n",
    "def _categorize_resources(all_resources: list[Resource]) -> dict[str, int]:\n",
    "    categories = {\"plates\": 0, \"reagents\": 0, \"locations\": 0, \"tips\": 0, \"other\": 0}\n",
    "    for resource in all_resources:\n",
    "        name = resource.resource_name.lower()\n",
    "        if \"plate\" in name and \"stack\" not in name:\n",
    "            categories[\"plates\"] += 1\n",
    "        elif any(\n",
    "            reagent in name\n",
    "            for reagent in [\"buffer\", \"dmso\", \"reagent\", \"trypsin\", \"media\"]\n",
    "        ):\n",
    "            categories[\"reagents\"] += 1\n",
    "        elif any(loc in name for loc in [\"deck\", \"hotel\", \"incubator\", \"refrigerator\"]):\n",
    "            categories[\"locations\"] += 1\n",
    "        elif \"tip\" in name:\n",
    "            categories[\"tips\"] += 1\n",
    "        else:\n",
    "            categories[\"other\"] += 1\n",
    "    return categories\n",
    "\n",
    "\n",
    "def _display_categories(categories: dict[str, int]) -> None:\n",
    "    for category, count in categories.items():\n",
    "        if count > 0:\n",
    "            print(f\"   • {category.title()}: {count}\")\n",
    "\n",
    "\n",
    "def _ownership_context_stats(all_resources: list[Resource]) -> None:\n",
    "    resources_with_context = len([r for r in all_resources if r.owner is not None])\n",
    "    context_percentage = (\n",
    "        (resources_with_context / len(all_resources)) * 100\n",
    "        if len(all_resources) > 0\n",
    "        else 0\n",
    "    )\n",
    "    print(\n",
    "        f\"   👥 Resources with ownership context: {resources_with_context}/{len(all_resources)} ({context_percentage:.0f}%)\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _sample_resource_validation(\n",
    "    client: ResourceClient, all_resources: list[Resource]\n",
    ") -> tuple[int, int]:\n",
    "    sample_resources = all_resources[:3]\n",
    "    valid_resources = 0\n",
    "    for resource in sample_resources:\n",
    "        with contextlib.suppress(Exception):\n",
    "            retrieved = client.get_resource(resource.resource_id)\n",
    "            if retrieved and retrieved.resource_id == resource.resource_id:\n",
    "                valid_resources += 1\n",
    "    print(\n",
    "        f\"   🔍 Resource retrieval validation: {valid_resources}/{len(sample_resources)} successful\"\n",
    "    )\n",
    "    return valid_resources, len(sample_resources)\n",
    "\n",
    "\n",
    "def validate_existing_resources() -> bool:\n",
    "    \"\"\"Validate that lab resources from setup are accessible.\"\"\"\n",
    "    if \"resource\" not in validator.clients:\n",
    "        print(\"   ❌ Resource client not available\")\n",
    "        return False\n",
    "\n",
    "    client = validator.clients[\"resource\"]\n",
    "\n",
    "    try:\n",
    "        all_resources = client.list_resources()\n",
    "        print(f\"   📊 Total resources found: {len(all_resources)}\")\n",
    "\n",
    "        if len(all_resources) == 0:\n",
    "            print(\"   ⚠️  No resources found - lab may need initial setup\")\n",
    "            return True\n",
    "\n",
    "        templates = client.list_templates()\n",
    "        print(f\"   📋 Templates available: {len(templates)}\")\n",
    "\n",
    "        categories = _categorize_resources(all_resources)\n",
    "        _display_categories(categories)\n",
    "        _ownership_context_stats(all_resources)\n",
    "        valid_resources, sample_size = _sample_resource_validation(\n",
    "            client, all_resources\n",
    "        )\n",
    "\n",
    "        return len(all_resources) > 0 and (valid_resources / sample_size) >= 0.5\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Resource validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run existing resources validation\n",
    "validator.run_test(\"Existing Lab Resources\", validate_existing_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. System Integration Test\n",
    "\n",
    "A comprehensive test that combines multiple services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System integration test\n",
    "def validate_system_integration() -> bool:\n",
    "    \"\"\"Comprehensive integration test across multiple services.\"\"\"\n",
    "    integration_steps_passed = 0\n",
    "    total_integration_steps = 4\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create a test experiment context\n",
    "        integration_context = OwnershipInfo(\n",
    "            node=\"integration_test\",\n",
    "            experiment=f\"integration_test_{int(time.time())}\",\n",
    "            user=\"validation_system\",\n",
    "        )\n",
    "        print(f\"   🔬 Test experiment: {integration_context.experiment}\")\n",
    "        integration_steps_passed += 1\n",
    "\n",
    "        # Step 2: Log integration test start event\n",
    "        if \"event\" in validator.clients:\n",
    "            event_client = validator.clients[\"event\"]\n",
    "            event_client.log_event(\n",
    "                event_type=\"integration_test_start\",\n",
    "                message=\"Starting system integration validation\",\n",
    "                context=MadsciContext(\n",
    "                    ownership=integration_context, lab_id=\"01JVDFED2K18FVF0E7JM7SX09F\"\n",
    "                ),\n",
    "            )\n",
    "            print(\"   📝 Integration start event logged\")\n",
    "            integration_steps_passed += 1\n",
    "        else:\n",
    "            print(\"   ⏭️  Event logging skipped (client not available)\")\n",
    "\n",
    "        # Step 3: Create and manage a test resource\n",
    "        if \"resource\" in validator.clients:\n",
    "            resource_client = validator.clients[\"resource\"]\n",
    "\n",
    "            # Create integration test resource\n",
    "            test_resource = Resource(\n",
    "                resource_name=f\"IntegrationTestResource_{int(time.time())}\",\n",
    "                resource_class=\"IntegrationTest\",\n",
    "                attributes={\n",
    "                    \"test_type\": \"system_integration\",\n",
    "                    \"created_at\": datetime.now().isoformat(),\n",
    "                },\n",
    "                owner=integration_context,\n",
    "                status=ResourceStatusEnum.in_use,\n",
    "            )\n",
    "\n",
    "            created_resource = resource_client.add_resource(test_resource)\n",
    "            resource_id = created_resource.resource_id\n",
    "            print(f\"   🧪 Test resource created: {created_resource.resource_name}\")\n",
    "\n",
    "            # Update the resource\n",
    "            resource_client.update_resource(\n",
    "                resource_id,\n",
    "                {\n",
    "                    \"attributes\": {\n",
    "                        **created_resource.attributes,\n",
    "                        \"integration_step\": \"validated\",\n",
    "                        \"updated_at\": datetime.now().isoformat(),\n",
    "                    },\n",
    "                    \"status\": ResourceStatusEnum.available,\n",
    "                },\n",
    "            )\n",
    "            print(\"   🔄 Test resource updated\")\n",
    "            integration_steps_passed += 1\n",
    "\n",
    "        else:\n",
    "            print(\"   ⏭️  Resource management skipped (client not available)\")\n",
    "\n",
    "        # Step 4: Log integration test completion\n",
    "        if \"event\" in validator.clients:\n",
    "            event_client.log_event(\n",
    "                event_type=\"integration_test_complete\",\n",
    "                message=f\"System integration validation completed - {integration_steps_passed} steps passed\",\n",
    "                context=MadsciContext(\n",
    "                    ownership=integration_context, lab_id=\"01JVDFED2K18FVF0E7JM7SX09F\"\n",
    "                ),\n",
    "            )\n",
    "            print(\"   ✅ Integration completion event logged\")\n",
    "            integration_steps_passed += 1\n",
    "\n",
    "        # Cleanup: Delete test resource\n",
    "        if \"resource\" in validator.clients and \"resource_id\" in locals():\n",
    "            try:\n",
    "                resource_client.delete_resource(resource_id)\n",
    "                print(\"   🧹 Test resource cleaned up\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Test resource cleanup failed: {e}\")\n",
    "\n",
    "        print(\n",
    "            f\"\\n   📊 Integration steps: {integration_steps_passed}/{total_integration_steps} completed\"\n",
    "        )\n",
    "        return integration_steps_passed >= (\n",
    "            total_integration_steps - 1\n",
    "        )  # Allow for one optional step to fail\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ System integration failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run system integration validation\n",
    "validator.run_test(\"System Integration\", validate_system_integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Summary and Report\n",
    "\n",
    "Let's generate a comprehensive validation report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate validation summary\n",
    "summary = validator.get_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔍 MADSCI LABORATORY VALIDATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📅 Validation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"⏱️  Total Duration: {summary['duration']:.2f} seconds\")\n",
    "print(\"🧪 Lab Context: validation_framework / system_validation\")\n",
    "\n",
    "print(\"\\n📊 Test Results Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total Tests: {summary['total']}\")\n",
    "print(f\"✅ Passed: {summary['passed']}\")\n",
    "print(f\"❌ Failed: {summary['failed']}\")\n",
    "print(f\"🚫 Errors: {summary['errors']}\")\n",
    "print(f\"📈 Success Rate: {summary['success_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\n🔍 Detailed Test Results:\")\n",
    "print(\"-\" * 35)\n",
    "for test_name, result in validator.results.items():\n",
    "    status_icon = {\"PASS\": \"✅\", \"FAIL\": \"❌\", \"ERROR\": \"🚫\"}.get(\n",
    "        result[\"status\"], \"❓\"\n",
    "    )\n",
    "\n",
    "    print(f\"{status_icon} {test_name}: {result['status']} ({result['duration']:.2f}s)\")\n",
    "\n",
    "    if result[\"status\"] == \"ERROR\" and \"error\" in result:\n",
    "        print(f\"    └─ Error: {result['error']}\")\n",
    "\n",
    "# Overall assessment\n",
    "print(\"\\n🎯 Overall Assessment:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "if summary[\"success_rate\"] >= 90:\n",
    "    assessment = \"🟢 EXCELLENT\"\n",
    "    message = \"Your MADSci laboratory is fully operational and ready for use!\"\n",
    "elif summary[\"success_rate\"] >= 75:\n",
    "    assessment = \"🟡 GOOD\"\n",
    "    message = (\n",
    "        \"Your MADSci laboratory is mostly operational with minor issues to address.\"\n",
    "    )\n",
    "elif summary[\"success_rate\"] >= 50:\n",
    "    assessment = \"🟠 NEEDS ATTENTION\"\n",
    "    message = \"Your MADSci laboratory has significant issues that should be resolved before production use.\"\n",
    "else:\n",
    "    assessment = \"🔴 CRITICAL ISSUES\"\n",
    "    message = (\n",
    "        \"Your MADSci laboratory has critical issues that must be addressed immediately.\"\n",
    "    )\n",
    "\n",
    "print(f\"Status: {assessment}\")\n",
    "print(f\"Message: {message}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n💡 Recommendations:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "if summary[\"failed\"] > 0 or summary[\"errors\"] > 0:\n",
    "    print(\"• Review failed tests above and address underlying issues\")\n",
    "    print(\"• Check service logs: docker compose logs [service-name]\")\n",
    "    print(\"• Verify all prerequisites are installed and configured\")\n",
    "\n",
    "if summary[\"success_rate\"] < 100:\n",
    "    print(\"• Consider re-running validation after addressing issues\")\n",
    "    print(\"• Check the troubleshooting section in previous notebooks\")\n",
    "\n",
    "if summary[\"success_rate\"] >= 75:\n",
    "    print(\"• Your lab setup is ready for tutorial workflows\")\n",
    "    print(\"• Consider exploring the scenarios/ directory for example use cases\")\n",
    "\n",
    "print(\"\\n📋 Next Steps:\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "if summary[\"success_rate\"] >= 90:\n",
    "    print(\"✅ Setup Complete! You can now:\")\n",
    "    print(\"   • Run the tutorial workflows in ../tutorials/\")\n",
    "    print(\"   • Explore example scenarios in ../scenarios/\")\n",
    "    print(\"   • Start developing your own laboratory workflows\")\n",
    "    print(\"   • Access the MADSci dashboard at http://localhost:8080\")\n",
    "elif summary[\"success_rate\"] >= 75:\n",
    "    print(\"⚠️  Partial Setup Complete:\")\n",
    "    print(\"   • Address the failed tests above\")\n",
    "    print(\"   • Re-run this validation notebook\")\n",
    "    print(\"   • Proceed with caution to tutorials\")\n",
    "else:\n",
    "    print(\"❌ Setup Issues Detected:\")\n",
    "    print(\"   • Review and fix all failing tests\")\n",
    "    print(\"   • Check service connectivity and configuration\")\n",
    "    print(\"   • Consider restarting services: just down && just up\")\n",
    "    print(\"   • Re-run previous setup notebooks if needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Validation report complete. Thank you for using MADSci!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Validation Report\n",
    "\n",
    "Let's save the validation report for record keeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export validation report\n",
    "def export_validation_report() -> Path | None:\n",
    "    \"\"\"Export validation report to JSON file.\"\"\"\n",
    "    try:\n",
    "        report_data = {\n",
    "            \"validation_metadata\": {\n",
    "                \"report_date\": datetime.now().isoformat(),\n",
    "                \"lab_id\": \"01JVDFED2K18FVF0E7JM7SX09F\",\n",
    "                \"validation_framework_version\": \"1.0\",\n",
    "                \"total_duration\": summary[\"duration\"],\n",
    "            },\n",
    "            \"summary\": summary,\n",
    "            \"test_results\": validator.results,\n",
    "            \"assessment\": {\n",
    "                \"overall_status\": assessment.split()[1],  # Extract status level\n",
    "                \"message\": message,\n",
    "                \"ready_for_production\": summary[\"success_rate\"] >= 90,\n",
    "            },\n",
    "            \"system_info\": {\n",
    "                \"validation_context\": validator.test_context.model_dump(),\n",
    "                \"services_tested\": [\n",
    "                    \"Event Manager\",\n",
    "                    \"Experiment Manager\",\n",
    "                    \"Resource Manager\",\n",
    "                    \"Data Manager\",\n",
    "                    \"Workcell Manager\",\n",
    "                ],\n",
    "                \"optional_components\": [\"Laboratory Nodes\", \"Workflow Execution\"],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Save to file\n",
    "        report_file = Path(\"../requirements/validation_report.json\")\n",
    "        report_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "        with Path(report_file).open(\"w\") as f:\n",
    "            json.dump(report_data, f, indent=2, default=str)\n",
    "\n",
    "        return report_file\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export validation report: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "exported_report = export_validation_report()\n",
    "\n",
    "if exported_report:\n",
    "    file_size = exported_report.stat().st_size / 1024\n",
    "    print(f\"\\n✅ Validation report exported to: {exported_report}\")\n",
    "    print(f\"   File size: {file_size:.1f} KB\")\n",
    "    print(f\"   Success rate: {summary['success_rate']:.1f}%\")\n",
    "    print(f\"   Tests completed: {summary['total']}\")\n",
    "else:\n",
    "    print(\"\\n❌ Failed to export validation report\")\n",
    "\n",
    "print(\n",
    "    \"\\n📋 Validation complete. Your MADSci laboratory setup has been thoroughly tested.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "🎉 **Validation Complete!** \n",
    "\n",
    "You have successfully validated your MADSci laboratory setup across multiple dimensions:\n",
    "\n",
    "### ✅ Completed Validation Areas:\n",
    "- **Service Health**: All MADSci microservices connectivity\n",
    "- **Database Connectivity**: PostgreSQL, MongoDB, Redis connections\n",
    "- **Client Initialization**: All MADSci Python clients\n",
    "- **Template Operations**: Complete template lifecycle\n",
    "- **Resource Lifecycle**: CRUD operations on lab resources\n",
    "- **Context Propagation**: Ownership and tracking across services\n",
    "- **Node Communication**: Laboratory instrument connectivity (optional)\n",
    "- **Workflow Execution**: Basic workflow submission (optional)\n",
    "- **Existing Resources**: Validation of provisioned lab resources\n",
    "- **System Integration**: End-to-end multi-service operations\n",
    "\n",
    "### 📊 What Was Tested:\n",
    "- Core service availability and health\n",
    "- Database connectivity and persistence\n",
    "- Resource management functionality\n",
    "- Template system operations\n",
    "- Context and ownership tracking\n",
    "- Inter-service communication\n",
    "- Data integrity and validation\n",
    "\n",
    "### 🔧 Validation Framework Features:\n",
    "- Automated test execution and timing\n",
    "- Comprehensive error handling and reporting\n",
    "- Detailed success/failure analysis\n",
    "- JSON report export for record keeping\n",
    "- Clear recommendations for next steps\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Setup Journey Complete!\n",
    "\n",
    "**Congratulations!** You have completed the MADSci laboratory setup journey:\n",
    "\n",
    "1. **✅ Service Orchestration** - Microservices architecture deployed\n",
    "2. **✅ Resource Templates** - Reusable resource definitions created\n",
    "3. **✅ Initial Resources** - Laboratory inventory provisioned\n",
    "4. **✅ Validation** - Comprehensive testing completed (this notebook)\n",
    "\n",
    "### 🚀 Ready for Next Steps:\n",
    "- **Interactive Tutorials**: `../tutorials/01_basic_setup/`\n",
    "- **Real-world Scenarios**: `../scenarios/pharmaceutical_screening/`\n",
    "- **Custom Workflows**: Develop your own laboratory automation\n",
    "- **Dashboard Access**: http://localhost:8080 (when available)\n",
    "\n",
    "### 💡 Pro Tips for Production Use:\n",
    "- Run validation regularly to catch issues early\n",
    "- Monitor service logs for performance insights\n",
    "- Export and archive validation reports for compliance\n",
    "- Set up automated health checks for continuous monitoring\n",
    "- Scale services based on actual laboratory workload\n",
    "\n",
    "**Your MADSci-powered laboratory is now ready for autonomous scientific discovery!** 🧬🤖"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
